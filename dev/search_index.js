var documenterSearchIndex = {"docs":
[{"location":"pages/other/#Other","page":"Other","title":"Other","text":"","category":"section"},{"location":"pages/other/","page":"Other","title":"Other","text":"currentvalue","category":"page"},{"location":"pages/other/#IMUDevNNTrainingLib.currentvalue","page":"Other","title":"IMUDevNNTrainingLib.currentvalue","text":"currentvalue(s::ParameterSchedulers.Stateful)\n\nReturn the current value of the scheduler s.\n\nwarning: Warning\nReally, this should be have been implemented somewhere in ParameterSchedulers.jl.\n\n\n\n\n\n","category":"function"},{"location":"pages/pretty_printing/#Pretty-printing","page":"Printing","title":"Pretty printing","text":"","category":"section"},{"location":"pages/pretty_printing/","page":"Printing","title":"Printing","text":"IMUDevNNTrainingLib implements a handful of useful pretty printing features. To accompany them there are a couple of routines that extract dimension/size information from the MLUtils.DataLoader holding data of the type TemporalData. These are:","category":"page"},{"location":"pages/pretty_printing/","page":"Printing","title":"Printing","text":"IMUDevNNTrainingLib.IMUDevNNLib.num_samples\nIMUDevNNTrainingLib.feature_dim\nIMUDevNNTrainingLib.target_dim\nIMUDevNNTrainingLib.batch_size","category":"page"},{"location":"pages/pretty_printing/#IMUDevNNLib.num_samples","page":"Printing","title":"IMUDevNNLib.num_samples","text":"IMUDevNNLib.num_samples(layout::SingleArrayLayout, loader::DataLoader)\n\nThe number of availble samples held by the loader (i.e. the maximal batch size).\n\n\n\n\n\n","category":"function"},{"location":"pages/pretty_printing/#IMUDevNNTrainingLib.feature_dim","page":"Printing","title":"IMUDevNNTrainingLib.feature_dim","text":"feature_dim(layout::SingleArrayLayout, loader::DataLoader)\n\nThe dimension of the feature space of the data held by the loader.\n\nnote: Note\nIf the state is a multidimensional tensor, feature_dim is equal to the total number of elements in a state tensor.\n\n\n\n\n\n","category":"function"},{"location":"pages/pretty_printing/#IMUDevNNTrainingLib.target_dim","page":"Printing","title":"IMUDevNNTrainingLib.target_dim","text":"target_dim(layout::SingleArrayLayout, loader::DataLoader)\n\nThe dimension of the target space of the data held by the loader. For TemporalData the target dimension is simply the size of the observation vector.\n\nnote: Note\nIf the observation is a multidimensional tensor, target_dim is equal to the total number of elements in an observation tensor.\n\n\n\n\n\n","category":"function"},{"location":"pages/pretty_printing/#IMUDevNNTrainingLib.batch_size","page":"Printing","title":"IMUDevNNTrainingLib.batch_size","text":"batch_size(loader::DataLoader)\n\nThe batch size of the loader.\n\n\n\n\n\n","category":"function"},{"location":"pages/pretty_printing/","page":"Printing","title":"Printing","text":"All the above, as well as some additional ones, can be extracted all at once from the loader either in a form of a NamedTuple or a formatted String using convenience methods:","category":"page"},{"location":"pages/pretty_printing/","page":"Printing","title":"Printing","text":"IMUDevNNTrainingLib.basic_info\nIMUDevNNTrainingLib.basic_info_as_string","category":"page"},{"location":"pages/pretty_printing/#IMUDevNNTrainingLib.basic_info","page":"Printing","title":"IMUDevNNTrainingLib.basic_info","text":"basic_info(layout::SingleArrayLayout, loader::DataLoader)\n\nReturn a NamedTuple with basic dimensional information about the loader and the data it holds.\n\n\n\n\n\n","category":"function"},{"location":"pages/pretty_printing/#IMUDevNNTrainingLib.basic_info_as_string","page":"Printing","title":"IMUDevNNTrainingLib.basic_info_as_string","text":"basic_info_as_string(loader::DataLoader)\n\nReturn a formatted string with basic dimensional information about the loader and the data it holds.\n\n\n\n\n\n","category":"function"},{"location":"pages/pretty_printing/","page":"Printing","title":"Printing","text":"However, all of the above are rarely used directly. Instead, they are being called by the method start_info (intended to be called at the beginning of training) that nicely formats the results:","category":"page"},{"location":"pages/pretty_printing/","page":"Printing","title":"Printing","text":"start_info","category":"page"},{"location":"pages/pretty_printing/#IMUDevNNTrainingLib.start_info","page":"Printing","title":"IMUDevNNTrainingLib.start_info","text":"start_info(layout::SingleArrayLayout, loader::DataLoader)\n\nPrint basic information about the loader and the data it holds.\n\ntip: Tip\nThis method is intended to be called at the start of training.\n\n\n\n\n\n","category":"function"},{"location":"pages/pretty_printing/","page":"Printing","title":"Printing","text":"Additionally, the following could be called during training to display progress:","category":"page"},{"location":"pages/pretty_printing/","page":"Printing","title":"Printing","text":"echo_epoch\necho_summary","category":"page"},{"location":"pages/pretty_printing/#IMUDevNNTrainingLib.echo_epoch","page":"Printing","title":"IMUDevNNTrainingLib.echo_epoch","text":"echo_epoch(id)\n\nPrint the current epoch number.\n\n\n\n\n\n","category":"function"},{"location":"pages/pretty_printing/#IMUDevNNTrainingLib.echo_summary","page":"Printing","title":"IMUDevNNTrainingLib.echo_summary","text":"echo_summary(; epoch, avg_loss, elapsed, title=\"Training\")\n\nPrint a summary of the training process.\n\n\n\n\n\n","category":"function"},{"location":"pages/checkpointer/#Checkpointing","page":"Checkpointer","title":"Checkpointing","text":"","category":"section"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"Lux is agnostic with respect to the way the user checkpoints the trained models. In particular the data-saving backend, the naming conventions and the choice of objects that need saving are all left up to the user. Although great from the perspective of flexibility, it calls for writing quite an extensive boilerplate code. IMUDevNNTrainingLib makes all those choices for the user and in return streamlines the checkpointing process. In particular, the following choices are made:","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"JLD2 is used as a backend for saving the state of the trained Neural Net\nA checkpoint comprises of saved:\nmodel_parameters: parameters of the trained Neural Network\nmodel_states: states (i.e. non-trainable parameters) of the trained Neural Network\nopt_state: parameters of the optimizer\nlog: a training log (with loss functions)\nother: a dictionary of any other parameters that the user wishes to save. For instance, plateau_detector may be saved here.\nNames of checkpoints take the format checkpoint=$(epoch).jld2, where $(epoch) is the index of the epoch at the end of which the checkpoint has been made.","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"The main object is Checkpointer:","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"Checkpointer","category":"page"},{"location":"pages/checkpointer/#IMUDevNNTrainingLib.Checkpointer","page":"Checkpointer","title":"IMUDevNNTrainingLib.Checkpointer","text":"Checkpointer(; dir::String = \".\",\n             continue_from::Union{Int,Symbol} = :last,\n             save_every::Int = 1)\n\nA struct to manage checkpoints during training of neural networks.\n\nnote: Note\nThe checkpointer is intended to save:model parameters\nmodel state\noptimizer state\ntraining log\nother objects passed by the userNote in particular that it is not recommended to save the model itself.\n\ndir::String: Root directory where the checkpoints will be saved to\ncontinue_from::Union{Int64, Symbol}: A default behaviour for resuming checkpointing. See start for details.\n\nsave_every::Int64: The frequency of saving checkpoints. A checkpoint will be saved every save_every epochs.\n\n!!! Tip \"Examples\"\n\n## Checkpointing during training\n```julia\nusing Lux, Optimisers\nusing Random\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\n# a function that updates the training log\nmodel = ...\nupdate_log!(...) = ...\nch = Checkpointer(dir=joinpath(homedir(), \"checkpoints\"),\n                  continue_from=:last,\n                  save_every=5)\nchkp_data, start_epoch = start(ch)\nparameters, states, opt_state, log, other = chkp_data\n\nfor epoch in start_epoch:100\n    train!(parameters, states, model, data, opt_state)\n    update_log!(log, model, data, epoch)\n    checkpoint(ch, epoch; parameters, states, opt_state, log, other)\nend\n```\n\n## Loading checkpoint for testing\n```julia\nmodel = Chain(...)\nch = Checkpointer(dir=joinpath(homedir(), \"checkpoints\"))\nchkp_data = load_checkpoint(model, path_to_last_checkpoint(ch))\ntest(model, chkp_data, data)\n```\n\n\n\n\n\n","category":"type"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"The usual training workflow involves starting it (which will load the specified checkpoint) as well as checkpointing it on every epoch, which will save the model when appropriate:","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"start\ncheckpoint","category":"page"},{"location":"pages/checkpointer/#IMUDevNNTrainingLib.start","page":"Checkpointer","title":"IMUDevNNTrainingLib.start","text":"start(cp::Checkpointer;\n      move_to_device=Lux.cpu_device(),\n      continue_from=cp.continue_from)\n\nStart the checkpointing process. continue_from can be one of the following:\n\n:none, :start, :restart, :nothing: for starting from scratch;\n:last, :latest, :recent: for continuing from the last checkpoint;\nInt: for continuing from a specific checkpoint index.\n\nThe move_to_device function is used to move the model parameters, model states and optimizer's state to the desired device (e.g. CPU or GPU). The function returns a NamedTuple with the checkpointed data (or nothing if no checkpoint is found) and an index of the subsequent training epoch.\n\n\n\n\n\n","category":"function"},{"location":"pages/checkpointer/#IMUDevNNTrainingLib.checkpoint","page":"Checkpointer","title":"IMUDevNNTrainingLib.checkpoint","text":"checkpoint(cp::Checkpointer, epoch::Int; model, opt_state, log, kwargs...)\n\nSave the model, the optimizer state and the training log to a checkpoint file if the given epoch is a multiple of cp.save_every. Otherwise, do nothing.\n\n\n\n\n\n","category":"function"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"note: Note\nSometimes the user may wish to save and, perhaps, update additional variables. For instance a common use case would be to save the state of the PlateauDetector. In that case the following could be done:model = ...\nch = Checkpointer()\nchkp_data, start_epoch = start(ch)\nif isnothing(chkp_data)\n    do_custom_initialization()\nend\npd = chkp_data.other[:plateau_detector]and if we'd like to start using a different schedule of learning rates we could adjust them with:new_schedule = ParameterSchedulers.Stateful(Exp(1e-3, 0.15))\nOptimisers.adjust!(chkp_data.opt_state, pd, new_schedule)before we resume the training.","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"For testing it is often enough to call load_checkpoint directly, instead of trying to establish the starting epoch as well:","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"load_checkpoint","category":"page"},{"location":"pages/checkpointer/#IMUDevNNTrainingLib.load_checkpoint","page":"Checkpointer","title":"IMUDevNNTrainingLib.load_checkpoint","text":"load_checkpoint(path::String; move_to_device=Lux.cpu_device())\n\nLoad the model parameters, model states, optimizer state, training log and the remaining variables from the checkpoint file. move_to_device is used to move the:\n\nmodel parameters\nmodel states and\noptimizer's state\n\nto the desired device (e.g. CPU or GPU).\n\n\n\n\n\n","category":"function"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"To pick the appropriate checkpoint one can pick from the following helper functions:","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"path_to_checkpoint\nindex_of_last_checkpoint\npath_to_last_checkpoint\nIMUDevNNTrainingLib.index_of_last_checkpoint_prior_to","category":"page"},{"location":"pages/checkpointer/#IMUDevNNTrainingLib.path_to_checkpoint","page":"Checkpointer","title":"IMUDevNNTrainingLib.path_to_checkpoint","text":"path_to_checkpoint(cp::Checkpointer, epoch::Int)\n\nReturn the path to the checkpoint file for the given epoch.\n\nwarning: Warning\nThis function does not check if the file exists.\n\n\n\n\n\n","category":"function"},{"location":"pages/checkpointer/#IMUDevNNTrainingLib.index_of_last_checkpoint","page":"Checkpointer","title":"IMUDevNNTrainingLib.index_of_last_checkpoint","text":"index_of_last_checkpoint(cp::Checkpointer)\n\nReturn the index of the most recent checkpoint in the directory cp.dir. Return nothing if no checkpoint exists.\n\n\n\n\n\n","category":"function"},{"location":"pages/checkpointer/#IMUDevNNTrainingLib.path_to_last_checkpoint","page":"Checkpointer","title":"IMUDevNNTrainingLib.path_to_last_checkpoint","text":"path_to_last_checkpoint(cp::Checkpointer)\n\nReturn the path to the most recent checkpoint file for the given epoch.\n\n\n\n\n\n","category":"function"},{"location":"pages/checkpointer/#IMUDevNNTrainingLib.index_of_last_checkpoint_prior_to","page":"Checkpointer","title":"IMUDevNNTrainingLib.index_of_last_checkpoint_prior_to","text":"index_of_last_checkpoint_prior_to(cp::Checkpointer, i::Int)\n\nReturn the index of the most recent checkpoint in the directory cp.dir out of checkpoints that are prior to the given index i. Return nothing if no such checkpoint exists.\n\n\n\n\n\n","category":"function"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"A typical example of loading for testing is given below:","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"# define model architecture\nmodel = ...\n\nchkp = Checkpointer(; dir=\"saved_checkpoints\")\nchkp_data = load_checkpoint(model, path_to_last_checkpoint(chkp))","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"To list available checkpoint indices you may use:","category":"page"},{"location":"pages/checkpointer/","page":"Checkpointer","title":"Checkpointer","text":"IMUDevNNTrainingLib.list_existing_checkpoint_indices","category":"page"},{"location":"pages/checkpointer/#IMUDevNNTrainingLib.list_existing_checkpoint_indices","page":"Checkpointer","title":"IMUDevNNTrainingLib.list_existing_checkpoint_indices","text":"list_existing_checkpoint_indices(cp::Checkpointer)\n\nList the epoch indices of the existing checkpoints in the directory cp.dir.\n\n\n\n\n\n","category":"function"},{"location":"#IMUDevNNTrainingLib.jl","page":"Home","title":"IMUDevNNTrainingLib.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"An extension of IMUDevNNLib to more training-oriented cases. It depends on slightly heavier packages, but in exchange, it simplifies some very common training workflows that we use across imu.dev.","category":"page"},{"location":"pages/plateau_detector/#Plateau-detector","page":"Plateau Detector","title":"Plateau detector","text":"","category":"section"},{"location":"pages/plateau_detector/","page":"Plateau Detector","title":"Plateau Detector","text":"It is often recommended to reduce the learning rate when the tracked loss metric has stopped improving for some period of time (for instance, in Python, various Neural Net libraries implement ReduceLROnPlateau, see keras's or pytorch's implementations). To this end we introduce a PlateauDetector that tracks the target loss metric and updates learning rate in case it stops improving for a sufficient number of ticks.","category":"page"},{"location":"pages/plateau_detector/","page":"Plateau Detector","title":"Plateau Detector","text":"note: Note\nWe will use the word tick to refer to a sequence of operations that results in PlateauDetector checking the loss metric precisely once. It is up to the user to decide how often that happens.","category":"page"},{"location":"pages/plateau_detector/","page":"Plateau Detector","title":"Plateau Detector","text":"note: Note\nUnlike python's ReduceLROnPlateau we don't restrict the reduction of the learning rate to be of the form of an exponential decay. Instead, we let the user choose the schedule along which the learning rate is being modified using the interface of Stateful defined in ParameterSchedulers.jl. In particular, an exponential decay could be defined via:using IMUDevNNTrainingLib\nusing ParameterSchedulers\nusing ParameterSchedulers: Stateful\n\ninit_learning_rate = 1e-3\ndecay = 0.7\nscheduler = Stateful(Exp(init_learning_rate, decay))\npd = PlateauDetector(; scheduler)","category":"page"},{"location":"pages/plateau_detector/","page":"Plateau Detector","title":"Plateau Detector","text":"PlateauDetector\nIMUDevNNTrainingLib.step!\nIMUDevNNTrainingLib.Optimisers.adjust!","category":"page"},{"location":"pages/plateau_detector/#IMUDevNNTrainingLib.PlateauDetector","page":"Plateau Detector","title":"IMUDevNNTrainingLib.PlateauDetector","text":"PlateauDetector(; patience=10, ϵ=1e-8,\n                  scheduler=ParameterSchedulers.Stateful(Exp(0.001, 0.7))\n\nA plateau detector for schedulers of learning rate. It will adjust the learning rate when the loss has stopped improving for a number of ticks.\n\ntip: Tip\nMost commonly one tick is equal either to one epoch or one batch, but the user is free to define it as they see fit.\n\npatience::Int64: The number of consecutive ticks with no improvement before advancing the learning rate along the scheduler.\n\nlast_tick::Int64: Internal only. The last tick number on which the learning rate was adjusted.\n\nbest_loss::Float64: Internal only. The best loss found so far.\n\nbest_tick::Int64: Internal only. The tick number of the best loss found so far.\n\nϵ::Any: The minimum learning rate. If the learning rate is reduced to a value smaller than this, it will be clipped at this value.\n\nscheduler::ParameterSchedulers.Stateful: The schedule along which the learning rate will be adjusted.\n\n\n\n\n\n","category":"type"},{"location":"pages/plateau_detector/#IMUDevNNTrainingLib.step!","page":"Plateau Detector","title":"IMUDevNNTrainingLib.step!","text":"step!(pd::PlateauDetector, loss::Real)\n\nAdvance the plateau detector by one tick. Update pd internals with information about the current loss. Return true if the learning rate should be updated.\n\n\n\n\n\n","category":"function"},{"location":"pages/plateau_detector/#Optimisers.adjust!","page":"Plateau Detector","title":"Optimisers.adjust!","text":"Optimisers.adjust!(opt_state, pd::PlateauDetector)\n\nUpdate the learning rate of the optimizer o according to the schedule defined by the plateau detector pd.\n\nOptimisers.adjust!(opt_state, pd::PlateauDetector, i::Int)\n\nReset the learning rate by moving to state i along the schedule defined by pd.scheduler.\n\nOptimisers.adjust!(opt_state, pd::PlateauDetector, s::ParameterSchedulers.Stateful)\n\nReset the learning rate by changing the scheduler to s and moving to the state defined by it.\n\ntip: Tip\nThese last two three-parameter functions are sometimes convenient when we want to manually restart the learning rate schedule (for instance, after loading a checkpoint and restarting training).\n\n\n\n\n\n","category":"function"},{"location":"pages/plateau_detector/","page":"Plateau Detector","title":"Plateau Detector","text":"warning: Warning\nTo avoid name clashes step! is not exported by IMUDevNNTrainingLib. It is recommended to use it as follows:using IMUDevNNTrainingLib\nusing Optimisers\nconst NNTrLib = IMUDevNNTrainingLib\npd = PlateauDetector(; η=1e-4)\noptimizer_state = ...\nloss = ...\nneeds_update = NNTrLib.step!(pd, loss)\nif needs_update\n    Optimisers.adjust!(optimizer_state, pd)\nend","category":"page"},{"location":"pages/plateau_detector/","page":"Plateau Detector","title":"Plateau Detector","text":"Additionally the following utility function is implemented:","category":"page"},{"location":"pages/plateau_detector/","page":"Plateau Detector","title":"Plateau Detector","text":"learning_rate","category":"page"},{"location":"pages/plateau_detector/#IMUDevNNTrainingLib.learning_rate","page":"Plateau Detector","title":"IMUDevNNTrainingLib.learning_rate","text":"learning_rate(pd::PlateauDetector)\n\nReturn the current learning rate of the plateau detector pd.\n\n\n\n\n\n","category":"function"},{"location":"pages/plateau_detector/","page":"Plateau Detector","title":"Plateau Detector","text":"An overloaded Optimisers.adjust! makes it possible to change the previously set schedules and adjust the state of the optimizer accordingly.","category":"page"},{"location":"pages/plateau_detector/","page":"Plateau Detector","title":"Plateau Detector","text":"See also the examples/plateau_detector.jl file for more details on how to use a PlateauDetector.","category":"page"}]
}
